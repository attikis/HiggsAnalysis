#!/bin/tcsh

# ------------------  modify these values --------------
# ------ Datacards
set combinedAnalysis = 0
# write name of fully hadronic datacard here (remove .txt)
set nameDataCard = lands_datacard_hplushadronic_m
# write names of other datacards in variables dc2/dc3/dc4
#
# ------ LSF jobs configurations
# quickest choice for non-crowded LSF:  125 jobs with 4 toy MCs
# use a bit fewer jobs in case LSF is crowded
# (number of toy MCs is set to 10 in mkBrLimits_submitExpSplit)
set nJobs = "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50"
#
# Use this to calculate approximate wall clock time (min) 
# and cpu time in root:
# n=60; m=8; cout << "time="<<n/60.0+3.6+0.82*(m) << ", tot ev="<<n*m<<", cpu="<<n*(3.6+0.82*m)
#
# ----- Masspoints: NOTE:hard-coded in plotting scripts also
set massPoints = "80 100 120 140 150 155 160"
# ----------------------------------------------

# do not modify these
set odir = `pwd`
set nameJobs = `basename ${odir}`
set jobname = ${nameJobs}_`date +"%H%M%S"`

if (${combinedAnalysis} == 1) then 
 echo "-------- Running combined analysis (remember to fix label in mkBrLimits_processBrPlots.cxx and mkBrLimits_processTanbPlots.py)"
else
 echo "-------- Running analysis for fully hadronic channel only"
endif
# check if jobs producing same output files are running
# NOTE: a job with longer name containing the name of this job might block this 
# job, even though it would not produce output files with same name
while (`bjobs -w |& grep $nameJobs` != "") 
  echo Waiting for previous jobs using this directory to end.  You might want to bkill these jobs...
  sleep 10
end

# prepare LandS
if ( !(-e mycmsswlink/src/UserCode) ) then
  cd mycmsswlink/src/
  cvs co -r V3-04-01_eps UserCode/mschen/LandS
  cd UserCode/mschen/LandS  
  cmsenv
  gmake clean
  gmake all
endif

cd ${odir}
mkdir outputs

rm split*.root

#Find out integrated luminosities from datacard files, one for each mass point
foreach xmas ($massPoints)
head -1 ${nameDataCard}${xmas}.txt | awk '{print strtonum(substr($7,12,5)) }' >&! outputs/input_luminosity_${xmas}
end

# jobs for expected limits, several jobs for each mass point 
# parameters for mkBrLimits_submitExpSplit:
# workdir, job number (seed), mass point, datacard file name(s)
# launch these first since obs jobs are much quicker to run
foreach xmas ($massPoints)
 foreach xjob ($nJobs)
  if (${combinedAnalysis} == 1) then
   set dc1 = ${nameDataCard}${xmas}.txt
   set dc2 = datacard_m${xmas}_emu_nobtag_dec14.txt
   set dc3 = datacard_m${xmas}_etau_miso_nov21.txt
   set dc4 = datacard_m${xmas}_mutau_miso_nov21.txt
  else
   set dc1 = ${nameDataCard}${xmas}.txt
   set dc2 = ""
   set dc3 = ""
   set dc4 = ""
  endif  
  bsub -o output_expected${xmas}_${xjob}.txt -q 1nh -J ${jobname}${xmas}_e_$xjob ${odir}/mkBrLimits_submitExpSplit ${odir} $xjob $xmas ${dc1} ${dc2} ${dc3} ${dc4}
 end
end

# jobs for observed limits, one for each mass point
foreach xmas ($massPoints)
 if (${combinedAnalysis} == 1) then
   set dc1 = ${nameDataCard}${xmas}.txt
   set dc2 = datacard_m${xmas}_emu_nobtag_dec14.txt 
   set dc3 = datacard_m${xmas}_etau_miso_nov21.txt 
   set dc4 = datacard_m${xmas}_mutau_miso_nov21.txt
 else
   set dc1 = ${nameDataCard}${xmas}.txt
   set dc2 = ""
   set dc3 = ""
   set dc4 = ""
 endif  
 bsub -o output_observed${xmas}.txt -q 1nh -J ${jobname}${xmas}_o ${odir}/mkBrLimits_submitObs ${odir} ${dc1} ${dc2} ${dc3} ${dc4}
end

# here check if jobs with same name are running
echo Reporting jobs of this directory with name $jobname.
while (`bjobs -w |& grep $jobname` != "") # check if job name exists
  echo `date +"%H:%M"` ": Waiting jobs, " `bjobs -w|grep ${jobname}|grep RUN|wc|awk '{print $1}'` " running, " `bjobs -w|grep ${jobname}|grep PEND|wc|awk '{print $1}'` " pending. (do no modify scripts yet.)"
  sleep 60
end

# todo : rename for mass
foreach xmas ($massPoints)
hadd split${xmas}_Combined_limits_tree.root split${xmas}_*limits_tree.root
./mycmsswlink/src/UserCode/mschen/LandS/test/lands.exe --doExpectation 1 --readLimitsFromFile split${xmas}_Combined_limits_tree.root | tail -5 >&! outputs/output_${nameDataCard}${xmas}_exp
end

# collect expected and observed values
foreach xmas ($massPoints)
head -3 outputs/output_${nameDataCard}${xmas}_obs | tail -1 | awk '{print $12 }' >&! outputs/output_${nameDataCard}${xmas}
head -3 outputs/output_${nameDataCard}${xmas}_exp | tail -1 | awk '{print $2 " " $3 " " $7 " " $5 " " $6}' >> outputs/output_${nameDataCard}${xmas}
end

cd ${odir}
rm -r LSFJOB_*
rm *Hybrid*root
rm output_??????ed*.txt

echo Ready with LSF jobs!
